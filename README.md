# ğŸ AI Snake Game - Deep Q-Learning & Hamiltonian Hybrid

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-green.svg)

An intelligent Snake game AI trained using **Deep Q-Learning (DQN)** with an optional **Hamiltonian Cycle safety net** for enhanced performance and guaranteed survival strategies.

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Architecture](#-architecture) â€¢ [Training](#-training) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ¯ Features

- **ğŸ§  Deep Q-Learning Agent**: Neural network-based AI that learns to play Snake through reinforcement learning
- **ğŸ”„ Hybrid Agent**: Combines DQN with Hamiltonian cycle algorithm as a safety fallback
- **ğŸ“Š Real-time Training Visualization**: Live plotting of scores and performance metrics
- **ğŸ’¾ Model Checkpointing**: Automatic saving of best-performing models with metadata
- **ğŸ® Interactive Demo**: Watch the AI play with visual indicators showing decision-making mode
- **ğŸ“ˆ Comprehensive Reward System**: Sophisticated reward shaping for faster learning
- **ğŸ§ª Testing Suite**: Multiple test files for validation and comparison

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.8+
pip
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ai-snake-game.git
cd ai-snake-game
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the training**
```bash
# Train standard DQN agent
python agent.py

# Train hybrid agent with Hamiltonian safety net
python agent_hybrid.py
```

4. **Watch a demo**
```bash
python demo_hybrid.py
```

---

## ğŸ“– Usage

### Training a New Agent

**Standard DQN Agent:**
```bash
python agent.py
```
- Trains a pure deep reinforcement learning agent
- Saves best models to `./model/`
- Displays live training metrics and plots

**Hybrid Agent:**
```bash
python agent_hybrid.py
```
- Combines DQN with Hamiltonian cycle fallback
- Switches to Hamiltonian path when danger is detected
- Saves models to `./model_hybrid/`
- Provides statistics on AI vs Hamiltonian decision usage

### Running Demos

```bash
# Visual demonstration of the hybrid agent
python demo_hybrid.py
```
- Green border: AI making decisions
- Yellow border: Hamiltonian cycle active
- Shows real-time score and decision mode

### Testing

```bash
# Run all tests
python tests/integration_test.py
python tests/test_hamiltonian.py
python tests/quick_visual_test.py

# Compare agent performance
python tests/compare_agents.py
```

---

## ğŸ—ï¸ Architecture

### Project Structure

```
ai-snake-game/
â”œâ”€â”€ ğŸ“„ Core Files
â”‚   â”œâ”€â”€ game.py                 # Snake game environment (Pygame)
â”‚   â”œâ”€â”€ model.py                # Neural network architecture (PyTorch)
â”‚   â”œâ”€â”€ agent.py                # Standard DQN agent
â”‚   â”œâ”€â”€ agent_hybrid.py         # Hybrid DQN + Hamiltonian agent
â”‚   â”œâ”€â”€ hamiltonian_path.py     # Hamiltonian cycle implementation
â”‚   â””â”€â”€ helper.py               # Plotting and utility functions
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ model/                  # Standard agent checkpoints
â”‚   â””â”€â”€ model_hybrid/           # Hybrid agent checkpoints
â”‚
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ integration_test.py     # Full system integration tests
â”‚   â”œâ”€â”€ test_hamiltonian.py     # Hamiltonian path tests
â”‚   â”œâ”€â”€ quick_visual_test.py    # Visual testing
â”‚   â””â”€â”€ compare_agents.py       # Agent performance comparison
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ REWARD_SYSTEM_EXPLAINED.md
â”‚   â”œâ”€â”€ HAMILTONIAN_INTEGRATION.md
â”‚   â”œâ”€â”€ COMPLETE_REWARD_SYSTEM.md
â”‚   â”œâ”€â”€ PERFORMANCE_BONUS_SYSTEM.md
â”‚   â”œâ”€â”€ QUICK_TUNE_GUIDE.md
â”‚   â”œâ”€â”€ RECORD_BONUS_SYSTEM.md
â”‚   â””â”€â”€ TAIL_REWARD_SYSTEM.md
â”‚
â”œâ”€â”€ ğŸ“ assets/
â”‚   â””â”€â”€ arial.ttf               # Font for game display
â”‚
â”œâ”€â”€ ğŸ® demo_hybrid.py           # Interactive demonstration
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ“ README.md                # This file
â””â”€â”€ ğŸš« .gitignore               # Git ignore rules
```

### Neural Network Architecture

**Standard Agent (11 inputs â†’ 256 hidden â†’ 3 outputs)**
```python
Input Layer: 11 features
â”œâ”€â”€ Danger detection (3): straight, right, left
â”œâ”€â”€ Direction state (4): up, down, left, right
â”œâ”€â”€ Food location (4): up, down, left, right

Hidden Layer: 256 neurons (ReLU activation)

Output Layer: 3 actions
â””â”€â”€ [Straight, Right Turn, Left Turn]
```

**Hybrid Agent (14 inputs â†’ 256 hidden â†’ 3 outputs)**
- Standard 11 features + 3 Hamiltonian features
- Additional inputs for safety assessment

---

## ğŸ§  How It Works

### Deep Q-Learning (DQN)

The agent uses **Q-Learning** with neural network approximation:

1. **State Perception**: Agent observes game state (11-14 features)
2. **Action Selection**: 
   - Early training: High exploration (random moves)
   - Late training: Exploitation (learned behavior)
3. **Reward Feedback**: Environment provides rewards/penalties
4. **Q-Value Update**: Network learns optimal action-value function
5. **Experience Replay**: Learns from past experiences stored in memory

**Key Equation:**
```
Q(s,a) = r + Î³ Ã— max Q(s',a')
```
Where:
- `Q(s,a)` = Quality of action `a` in state `s`
- `r` = Immediate reward
- `Î³ = 0.9` = Discount factor (values future rewards)
- `max Q(s',a')` = Best future Q-value

### Reward System

| Event | Reward | Purpose |
|-------|--------|---------|
| ğŸ Eat Food | +10 | Primary objective |
| â¡ï¸ Move Closer to Food | +1 | Guide towards food |
| â¬…ï¸ Move Away from Food | -1 | Discourage wandering |
| ğŸ’€ Die (collision) | -10 | Avoid death |

### Hybrid System

The **Hybrid Agent** combines two strategies:

1. **DQN Mode** (Default): AI makes learned decisions
2. **Hamiltonian Mode** (Safety): Follows guaranteed survival path

**Activation Conditions:**
- Safety score < 30%
- Stuck in dangerous position
- High collision risk detected

**Benefits:**
- Guaranteed survival in critical situations
- Faster learning (safer exploration)
- Higher average scores

---

## ğŸ“Š Training

### Training Parameters

```python
MAX_MEMORY = 100,000      # Experience replay buffer size
BATCH_SIZE = 1,000        # Training batch size
LEARNING_RATE = 0.001     # Neural network learning rate
GAMMA = 0.9               # Discount factor for future rewards
EPSILON = 80 - n_games    # Exploration rate (decreases over time)
```

### Training Progress

The agent typically shows improvement in stages:

| Games | Avg Score | Behavior |
|-------|-----------|----------|
| 0-50 | 0-5 | Random exploration, frequent deaths |
| 50-100 | 5-15 | Learning food direction |
| 100-200 | 15-30 | Avoiding walls, basic strategy |
| 200+ | 30-60+ | Advanced patterns, survival |

Best recorded scores: **60+** (Standard), **Higher with Hybrid**

### Monitoring Training

- **Live Plot**: Real-time score visualization
- **Console Output**: Game number, score, record, mean score
- **Model Checkpoints**: Saved after each new record
- **Metadata**: Training statistics saved with models

---

## ğŸ”§ Configuration

### Adjusting Game Speed

In `game.py`:
```python
SPEED = 5000  # Higher = slower gameplay
```

### Modifying Reward Values

In `game.py`, locate the reward calculation:
```python
reward = 0
if game_over:
    reward = -10
    return reward, game_over, self.score

if self.score > score:
    reward = 10
```

### Tuning Learning Parameters

See [docs/QUICK_TUNE_GUIDE.md](docs/QUICK_TUNE_GUIDE.md) for detailed parameter tuning.

---

## ğŸ“š Documentation

Comprehensive documentation available in `/docs`:

- **[REWARD_SYSTEM_EXPLAINED.md](docs/REWARD_SYSTEM_EXPLAINED.md)** - Complete reward system breakdown
- **[HAMILTONIAN_INTEGRATION.md](docs/HAMILTONIAN_INTEGRATION.md)** - Hybrid system architecture
- **[QUICK_TUNE_GUIDE.md](docs/QUICK_TUNE_GUIDE.md)** - Parameter tuning guide
- **[COMPLETE_REWARD_SYSTEM.md](docs/COMPLETE_REWARD_SYSTEM.md)** - Advanced reward strategies
- **[PERFORMANCE_BONUS_SYSTEM.md](docs/PERFORMANCE_BONUS_SYSTEM.md)** - Performance optimization
- **[RECORD_BONUS_SYSTEM.md](docs/RECORD_BONUS_SYSTEM.md)** - Record tracking system
- **[TAIL_REWARD_SYSTEM.md](docs/TAIL_REWARD_SYSTEM.md)** - Tail avoidance strategies

---

## ğŸ® Controls & Visualization

### Game Display

- **Blue Segments**: Snake body
- **Red Square**: Food
- **White Background**: Play area
- **Score Display**: Top of window

### Demo Mode Indicators

- **Green Border**: AI in control (DQN)
- **Yellow Border**: Hamiltonian safety mode active

---

## ğŸ§ª Testing

### Available Tests

```bash
# Integration test - full system check
python tests/integration_test.py

# Hamiltonian path validation
python tests/test_hamiltonian.py

# Quick visual verification
python tests/quick_visual_test.py

# Compare standard vs hybrid agents
python tests/compare_agents.py
```

---

## ğŸ“ˆ Performance

### Benchmarks

**Standard DQN Agent:**
- Average Score (200+ games): 25-35
- Best Score: 60+
- Training Time: ~30-60 minutes to convergence

**Hybrid Agent:**
- Average Score (200+ games): 30-45
- Best Score: 65+
- Training Time: ~45-75 minutes to convergence
- Hamiltonian Usage: 10-20% of decisions

*Benchmarks based on default parameters on consumer hardware*

---

## ğŸ› ï¸ Technologies Used

- **[Python 3.8+](https://www.python.org/)** - Core language
- **[PyTorch](https://pytorch.org/)** - Deep learning framework
- **[Pygame](https://www.pygame.org/)** - Game engine and visualization
- **[NumPy](https://numpy.org/)** - Numerical computations
- **[Matplotlib](https://matplotlib.org/)** - Training visualization

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Areas for Contribution

- ğŸ¯ New reward strategies
- ğŸ§  Alternative neural network architectures
- ğŸ“Š Enhanced visualization
- ğŸ§ª Additional test cases
- ğŸ“š Documentation improvements
- ğŸ® UI/UX enhancements

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Inspired by classic Snake game implementations
- Deep Q-Learning algorithm from DeepMind's DQN paper
- Hamiltonian cycle approach for guaranteed survival
- Pygame community for excellent documentation

---

## ğŸ“§ Contact

For questions, suggestions, or issues:

- **GitHub Issues**: [Create an issue](https://github.com/yourusername/ai-snake-game/issues)
- **Email**: your.email@example.com

---

## ğŸ”® Future Enhancements

- [ ] Multi-agent training environment
- [ ] Convolutional Neural Network (CNN) for visual input
- [ ] Dueling DQN architecture
- [ ] Prioritized experience replay
- [ ] Web-based demo interface
- [ ] Tournament mode for agent comparison
- [ ] Save/load training sessions
- [ ] Configurable game board sizes

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star! â­**

Made with â¤ï¸ and ğŸ

</div>
